{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import norm, pearsonr\n",
    "from scipy.integrate import quad\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage.metrics import structural_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "from matplotlib.patches import Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesee.evaluation import *\n",
    "from bayesee.perception import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.style.use('bayesee.academic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path.cwd().parents[0]\n",
    "print(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"PA\"\n",
    "file_name = repo_path / f\"data/search/covert/large-field/p3_data_{subject}.pickle\"\n",
    "\n",
    "with open(file_name, \"rb\") as f:\n",
    "    stimulus, response = pickle.load(f)\n",
    "\n",
    "metadata = stimulus[\"metadata\"]\n",
    "spot_centers = metadata[\"spot_centers\"].astype(int)\n",
    "monitor_width, monitor_height = metadata[\"monitor_size\"]\n",
    "stimulus_size = metadata[\"stimulus_size\"]\n",
    "n_location = metadata[\"n_location\"]\n",
    "spot_size = metadata[\"spot_size\"]\n",
    "stimulus_ppd = metadata[\"stimulus_ppd\"]\n",
    "target_amplitude = metadata[\"target_amplitude\"]\n",
    "target = metadata[\"target\"]\n",
    "\n",
    "file_name = (\n",
    "    repo_path / f\"data/search/covert/large-field/p2_spatial_statistics_{subject}.csv\"\n",
    ")\n",
    "spatial_statistics_human = pd.read_csv(file_name)\n",
    "local_dp = spatial_statistics_human[\"dp\"].values\n",
    "local_ecc = spatial_statistics_human['ecc'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_target = stimulus[\"df\"][\"location\"].values\n",
    "human_response = response[\"df\"][\"response_location\"].values\n",
    "human_accuracy = human_target == human_response\n",
    "local_ecc = spatial_statistics_human['ecc'].values\n",
    "array_index_sort = np.insert(np.argsort(local_ecc)+1,0,0)\n",
    "human_confusion = confusion_matrix(human_response, human_target, labels=array_index_sort)\n",
    "human_cr_all = human_confusion[0,0] / sum(human_target==0)\n",
    "human_hit = np.array([human_confusion[l,l] / sum(human_target == array_index_sort[l]) for l in range(1, n_location)])\n",
    "human_fa = human_confusion[1:,0] / sum(human_target==0)\n",
    "human_dp = norm.ppf(human_hit) - norm.ppf(1 - human_cr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_location_ring = np.array([1,1,6,12])\n",
    "\n",
    "def confusion_matrix_ring(confusion_mat, n_location_ring):\n",
    "    # confusion matrix has been sorted by ring\n",
    "    n_ring = len(n_location_ring) - 1\n",
    "    cumsum_ring = np.cumsum(n_location_ring)\n",
    "    \n",
    "    confusion_ring = np.zeros(4*n_ring+1)\n",
    "    confusion_diagonal = confusion_mat.diagonal()\n",
    "    confusion_ring[0] = confusion_diagonal[0]\n",
    "\n",
    "    for index_ring in range(n_ring):\n",
    "        array_index_ring = range(cumsum_ring[index_ring], cumsum_ring[index_ring+1])\n",
    "        hit_ring = confusion_diagonal[array_index_ring].sum()\n",
    "        miss_ring = confusion_mat[0,array_index_ring].sum()\n",
    "        fa_ring = confusion_mat[array_index_ring,0].sum()\n",
    "        fhf_ring = confusion_mat[array_index_ring, 1:].sum() - hit_ring\n",
    "        confusion_ring[1+index_ring*4:5+index_ring*4] = hit_ring, miss_ring, fa_ring, fhf_ring\n",
    "    \n",
    "    return confusion_ring\n",
    "\n",
    "human_confusion_ring = confusion_matrix_ring(human_confusion, n_location_ring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortical_ppd = 60\n",
    "retinal_ppd = 60\n",
    "\n",
    "file_name = repo_path / f\"data/search/cortical/xij_{cortical_ppd}ppd.csv\"\n",
    "xij = np.genfromtxt(file_name, delimiter=\",\")\n",
    "\n",
    "file_name = repo_path / f\"data/search/cortical/yij_{cortical_ppd}ppd.csv\"\n",
    "yij = np.genfromtxt(file_name, delimiter=\",\")\n",
    "\n",
    "file_name = repo_path / f\"data/search/cortical/ixy_{retinal_ppd}ppd.csv\"\n",
    "ixy = np.genfromtxt(file_name, delimiter=\",\")\n",
    "\n",
    "file_name = repo_path / f\"data/search/cortical/jxy_{retinal_ppd}ppd.csv\"\n",
    "jxy = np.genfromtxt(file_name, delimiter=\",\")\n",
    "\n",
    "cut_off = 8  # degree\n",
    "xij = xij[:, : cut_off * cortical_ppd].copy()\n",
    "yij = yij[:, : cut_off * cortical_ppd].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xij_ppd = xij * retinal_ppd\n",
    "yij_ppd = yij * retinal_ppd\n",
    "ixy_ppd = ixy * cortical_ppd\n",
    "jxy_ppd = jxy * cortical_ppd\n",
    "\n",
    "mesh_y, mesh_x = np.meshgrid(\n",
    "    np.arange(int(yij_ppd.min()), int(yij_ppd.max())),\n",
    "    np.arange(int(xij_ppd.min()), int(xij_ppd.max())),\n",
    ")\n",
    "\n",
    "spot_centers_cortical = np.zeros_like(spot_centers)\n",
    "for i in range(spot_centers.shape[0]):\n",
    "    x, y = spot_centers[i] - spot_centers[0]\n",
    "    if y >= 0:\n",
    "        index_loc = (mesh_x == x) & (mesh_y == y)\n",
    "    else:\n",
    "        index_loc = (mesh_x == x) & (mesh_y == -y)\n",
    "        \n",
    "    spot_centers_cortical[i, :] = ixy_ppd[index_loc][0], jxy_ppd[index_loc][0]\n",
    "\n",
    "cortical_gain = np.zeros_like(xij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array((0.5, *((0.5 / (n_location - 1),) * (n_location - 1))))\n",
    "assert np.allclose(prior.sum(), 1.0)\n",
    "e_prior = prior.copy()\n",
    "assert np.allclose(e_prior.sum(), 1.0)\n",
    "log_prior_ratio = np.log(e_prior / e_prior[0])\n",
    "log_likelihood_ratio = np.zeros_like(prior)\n",
    "\n",
    "model_response = np.zeros_like(human_target)\n",
    "n_bootstrap = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x):\n",
    "    # x[0]: fovea_gain\n",
    "    # x[1]: periphery_gain\n",
    "    # x[2]: Weibull scale parameter a\n",
    "    # x[3]: Weibull shape parameter b\n",
    "    # x[4]: d' peak\n",
    "    # x[5]: d' fall-off\n",
    "\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    cost = 0\n",
    "    for index_i in np.arange(xij.shape[1]):\n",
    "        cortical_gain[:, index_i] = x[0]+ (x[1] - x[0]) * (1 - np.exp(-((index_i / (cortical_ppd * x[2])) ** x[3])))\n",
    "        \n",
    "    local_gain = np.array(\n",
    "        [\n",
    "            cortical_gain[*spot_centers_cortical[n_location]]\n",
    "            for n_location in range(n_location - 1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    local_gdp = local_dp * local_gain\n",
    "    e_local_dp = x[4] * x[5] / (local_ecc + x[5])\n",
    "    \n",
    "    for index_bootstrap in range(n_bootstrap):\n",
    "        for index_trial in range(len(human_target)):\n",
    "\n",
    "            array_standard_normal = rng.normal(size=(n_location - 1,))\n",
    "            \n",
    "            log_likelihood_ratio[1:] = e_local_dp * (\n",
    "                array_standard_normal - e_local_dp / 2\n",
    "            )\n",
    "\n",
    "            if human_target[index_trial] > 0:\n",
    "                log_likelihood_ratio[human_target[index_trial]] += (\n",
    "                    e_local_dp[human_target[index_trial] - 1]\n",
    "                    * local_gdp[human_target[index_trial] - 1]\n",
    "                )\n",
    "\n",
    "            log_posterior_ratio = log_prior_ratio + log_likelihood_ratio\n",
    "            model_response[index_trial] = np.argmax(log_posterior_ratio)\n",
    "            \n",
    "        model_confusion = confusion_matrix(model_response, human_target, labels=array_index_sort) \n",
    "        model_confusion_probability = (model_confusion + 1) # Laplace smoothing\n",
    "        model_confusion_probability = model_confusion_probability / model_confusion_probability.sum()\n",
    "        \n",
    "        model_confusion_ring = confusion_matrix_ring(model_confusion, n_location_ring)\n",
    "        model_confusion_ring_probability = (model_confusion_ring + 1) # Laplace smoothing\n",
    "        model_confusion_ring_probability = model_confusion_ring_probability / model_confusion_ring_probability.sum()\n",
    "        \n",
    "        model_cr_all = model_confusion[0,0] / sum(human_target==0)\n",
    "        model_hit = np.array([model_confusion[l,l] / sum(human_target == array_index_sort[l]) for l in range(1, n_location)])\n",
    "        model_fa = model_confusion[1:,0] / sum(human_target==0)\n",
    "        model_dp = norm.ppf(model_hit) - norm.ppf(1 - model_cr_all)\n",
    "        \n",
    "        # cost += rmse(human_confusion, model_confusion)\n",
    "        # cost += mae(human_confusion, model_confusion)\n",
    "        # cost += -(human_confusion * np.log(model_confusion_probability)).sum()\n",
    "        # cost += - pearson(human_confusion, model_confusion)\n",
    "        # cost += - ssim(human_confusion, model_confusion)\n",
    "        # cost += - cosine_similarity(human_confusion, model_confusion)\n",
    "        \n",
    "        # cost += rmse(human_confusion_ring, model_confusion_ring)\n",
    "        # cost += mae(human_confusion_ring, model_confusion_ring)\n",
    "        cost += -(human_confusion_ring * np.log(model_confusion_ring_probability)).sum()\n",
    "        # cost += - pearson(human_confusion_ring, model_confusion_ring)\n",
    "        # cost += - ssim(human_confusion_ring, model_confusion_ring)\n",
    "        # cost += - cosine_similarity(human_confusion_ring, model_confusion_ring)\n",
    "        \n",
    "        # cost += rmse(human_confusion.diagonal(), model_confusion.diagonal())\n",
    "        # cost += mae(human_confusion.diagonal(), model_confusion.diagonal())\n",
    "        # cost += -(human_confusion.diagonal() * np.log(model_confusion_probability.diagonal())).sum()\n",
    "        # cost += - pearson(human_confusion.diagonal(), model_confusion.diagonal())\n",
    "        # cost += - ssim(human_confusion.diagonal(), model_confusion.diagonal())\n",
    "        # cost += - cosine_similarity(human_confusion.diagonal(), model_confusion.diagonal())\n",
    "        \n",
    "        # cost += rmse(human_dp, model_dp)\n",
    "        # cost += mae(human_dp, model_dp)\n",
    "        # cost += - pearson(human_dp, model_dp)\n",
    "        # cost += - ssim(human_dp, model_dp)\n",
    "        # cost += - cosine_similarity(human_dp, model_dp)\n",
    "        \n",
    "        # cost += rmse(human_hit, model_hit)\n",
    "        # cost += mae(human_hit, model_hit)\n",
    "        # cost += - pearson(human_hit, model_hit)\n",
    "        # cost += - ssim(human_hit, model_hit)\n",
    "        # cost += - cosine_similarity(human_hit, model_hit)\n",
    "    \n",
    "        # cost += rmse(human_fa, model_fa)\n",
    "        # cost += mae(human_fa, model_fa)\n",
    "        # cost += - pearson(human_fa, model_fa)\n",
    "        # cost += - ssim(human_fa, model_fa)\n",
    "        # cost += - cosine_similarity(human_fa, model_fa)\n",
    "    \n",
    "    return cost / n_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results = pd.DataFrame(columns=[\"gf\", \"gp\", \"a\", \"b\", \"dp0\", \"ed\", \"cost\", \"nfunc\", \"niter\"])\n",
    "bounds = [(0.5, 1.0), (0.5, 1.5), (1,4), (1,3), (2,7), (2, 16)]\n",
    "n_init = 5\n",
    "rng_seed = 2\n",
    "\n",
    "rng = np.random.default_rng(rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_init in range(n_init):\n",
    "    init = [(bound[1] - bound[0]) * rng.random() + bound[0] for bound in bounds]\n",
    "    \n",
    "    results = minimize(\n",
    "        cost_function,\n",
    "        init,\n",
    "        bounds=bounds,\n",
    "        method=\"Powell\",\n",
    "    )\n",
    "    \n",
    "    row_result = [ *results.x, results.fun, results.nfev, results.nit]    \n",
    "    \n",
    "    if index_init == 0:\n",
    "        opt_results.loc[0] = row_result\n",
    "    elif results.fun < opt_results.iloc[-1][\"cost\"]:\n",
    "        opt_results.loc[len(opt_results)] = row_result\n",
    "        \n",
    "    print(f\"{index_init+1}/{n_init}\")\n",
    "\n",
    "row_result = opt_results.iloc[-1].values\n",
    "print(f\"| {row_result[-3]:.5f} | {row_result[0]:.3f} | {row_result[1]:.3f} | {row_result[2]:.3f} | {row_result[3]:.3f} | {row_result[4]:.3f} | {row_result[5]:.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trial = 10000\n",
    "rng = np.random.default_rng(rng_seed)\n",
    "model_target = rng.choice(range(n_location), size=n_trial, p=prior)\n",
    "model_response = np.zeros_like(model_target)\n",
    "\n",
    "stimulus_df = pd.DataFrame()\n",
    "response_df = pd.DataFrame()\n",
    "model = 'gainer37'\n",
    "metadata['gf'] = gf = 0.780\n",
    "metadata['gp'] = gp = 1.348\n",
    "metadata['a'] = a = 3.781\n",
    "metadata['b'] = b = 2.580\n",
    "metadata['dp0'] = dp0 = 3.547\n",
    "metadata['ed'] = ed = 7.054\n",
    "\n",
    "for index_i in np.arange(xij.shape[1]):\n",
    "    cortical_gain[:, index_i] = gf + (gp - gf) * (1 - np.exp(-((index_i / (cortical_ppd * a)) ** b)))\n",
    "    \n",
    "local_gain = np.array(\n",
    "    [\n",
    "        cortical_gain[*spot_centers_cortical[n_location]]\n",
    "        for n_location in range(n_location - 1)\n",
    "    ]\n",
    ")\n",
    "    \n",
    "local_gdp = local_dp * local_gain\n",
    "e_local_dp = dp0 * ed / (local_ecc + ed)\n",
    "\n",
    "for index_bootstrap in range(n_bootstrap):\n",
    "    for index_trial in range(len(model_target)):\n",
    "\n",
    "        array_standard_normal = rng.normal(size=(n_location - 1,))\n",
    "        \n",
    "        log_likelihood_ratio[1:] = e_local_dp * (\n",
    "            array_standard_normal - e_local_dp / 2\n",
    "        )\n",
    "\n",
    "        if model_target[index_trial] > 0:\n",
    "            log_likelihood_ratio[model_target[index_trial]] += (\n",
    "                e_local_dp[model_target[index_trial] - 1]\n",
    "                * local_gdp[model_target[index_trial] - 1]\n",
    "            )\n",
    "\n",
    "        log_posterior_ratio = log_prior_ratio + log_likelihood_ratio\n",
    "        model_response[index_trial] = np.argmax(log_posterior_ratio)\n",
    "    \n",
    "stimulus_df['location'] = model_target.round()\n",
    "response_df['response_location'] = model_response.round()\n",
    "    \n",
    "file_name = repo_path / f\"data/search/covert/large-field/p3_data_model_{model}_{subject}.pickle\"\n",
    "stimulus = {\"df\": stimulus_df, \"metadata\": metadata}\n",
    "response = {\"df\": response_df}\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    pickle.dump((stimulus, response), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
